# bisecting-kmeans-implementation

K-means. This is a prototype-based, partitional clustering technique that attempts to find a user-specified number of clusters (K), which are represented by their centroids. Using elbow method, we can find better K value (From where the SSE is linearly decreasing) to apply k-clustering.
The total SSE for k = 5 that we got by just applying Kmeans = 4.685571e+18. 

Bisecting K-means is another approach that speeds up K-means by reducing the number of similarities computed. It is also one of the approaches that are also useful for producing better-quality (lower SSE) clustering’s.

The bisecting K-means algorithm is a straightforward extension of the basic K-means algorithm that is based on a simple idea: to obtain K clusters, split the set of all points into two clusters, select one of these clusters to split, and so on, until K clusters have been produced. The details/Implementation of bisecting K-means are given in the above 3.4 problem. There are a number of different ways to choose which cluster to split. We can choose the largest cluster at each step, choose the one with the largest SSE, or use a criterion based on both size and SSE. Different choices result in different clusters. Because we are using the K-means algorithm “locally,” i.e., to bisect individual clusters, the final set of clusters does not represent a clustering that is a local minimum with respect to the total SSE. Thus, we often refine the resulting clusters by using their cluster centroids as the initial centroids for the standard K-means algorithm.
The total SSE for k=5 that we got by applying Bisecting = 4.876884e+18. As we see we got more than the k means SSE. But approximately near. But still there is a difference in the clusters as we can see.   

Agglomerative Hierarchical Clustering. This clustering approach refers to a collection of closely related clustering techniques that produce a hierarchical clustering by starting with each point as a singleton cluster and then repeatedly merging the two closest clusters until a single, Allen compassing cluster remains. Some of these techniques have a natural interpretation in terms of graph-based clustering, while others have an interpretation in terms of a prototype-based approach. The total SSE that we got is 3.138043e+19. 

From the Implementation point of view that agglomerative hierarchical clustering cannot be viewed as globally optimizing an objective function. More generally, such algorithms are typically used because the underlying application, Also, some studies have suggested that these algorithms can produce better-quality clusters. However, agglomerative hierarchical clustering algorithms are expensive in terms of their computational and storage requirements. The fact that all merges are final can also cause trouble for noisy, high-dimensional data, such as document data. In turn, these two problems can be addressed to some degree by first partially clustering the data using another technique, such as K-means.

